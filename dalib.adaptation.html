

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>DALIB Algorithms &mdash; dalib 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Vision Datasets" href="dalib.vision.datasets.html" />
    <link rel="prev" title="DALIB Basic Modules" href="dalib.modules.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> dalib
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="dalib.modules.html">DALIB Basic Modules</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">DALIB Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#dan">DAN</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dann">DANN</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cdan">CDAN</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mdd">MDD</a></li>
<li class="toctree-l2"><a class="reference internal" href="#afn">AFN</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dalib.vision.datasets.html">Vision Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="dalib.vision.models.html">Vision Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="dalib.text.datasets.html">Text Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="dalib.text.models.html">Text Models</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">dalib</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>DALIB Algorithms</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/dalib.adaptation.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="dalib-algorithms">
<h1>DALIB Algorithms<a class="headerlink" href="#dalib-algorithms" title="Permalink to this headline">¶</a></h1>
<div class="section" id="dan">
<h2>DAN<a class="headerlink" href="#dan" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="dalib.adaptation.dan.MultipleKernelMaximumMeanDiscrepancy">
<em class="property">class </em><code class="sig-prename descclassname">dalib.adaptation.dan.</code><code class="sig-name descname">MultipleKernelMaximumMeanDiscrepancy</code><span class="sig-paren">(</span><em class="sig-param">kernels</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalib/adaptation/dan.html#MultipleKernelMaximumMeanDiscrepancy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalib.adaptation.dan.MultipleKernelMaximumMeanDiscrepancy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The Multiple Kernel Maximum Mean Discrepancy (MK-MMD) used in
<a class="reference external" href="https://arxiv.org/pdf/1502.02791">Learning Transferable Features with Deep Adaptation Networks</a></p>
<dl>
<dt>Parameters:</dt><dd><ul class="simple">
<li><p>kernels (list of class:<cite>nn.Module</cite> object): multiple kernels to compute the domain discrepancy</p></li>
</ul>
</dd>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>f_s, f_t: <span class="math notranslate nohighlight">\((N, F)\)</span> where F means the dimension of input features.</p></li>
<li><p>Output: scalar</p></li>
</ul>
</dd>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">feature_dim</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernels</span> <span class="o">=</span> <span class="p">[</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">GaussianKernel</span><span class="p">(</span><span class="mf">1.</span><span class="p">),</span> <span class="n">GaussianKernel</span><span class="p">(</span><span class="mf">2.</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">adaptation</span><span class="o">.</span><span class="n">dan</span><span class="o">.</span><span class="n">MultipleKernelMaximumMeanDiscrepancy</span><span class="p">(</span><span class="n">kernels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># features from source domain and target domain</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f_s</span><span class="p">,</span> <span class="n">f_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">f_s</span><span class="p">,</span> <span class="n">f_t</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="dalib.adaptation.dan.GaussianKernel">
<em class="property">class </em><code class="sig-prename descclassname">dalib.adaptation.dan.</code><code class="sig-name descname">GaussianKernel</code><span class="sig-paren">(</span><em class="sig-param">sigma=None</em>, <em class="sig-param">momentum=0.1</em>, <em class="sig-param">track_running_stats=True</em>, <em class="sig-param">alpha=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalib/adaptation/dan.html#GaussianKernel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalib.adaptation.dan.GaussianKernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Gaussian Kernel Matrix</p>
<p>Gaussian Kernel k is defined by</p>
<div class="math notranslate nohighlight">
\[k(x_1, x_2) = \exp \left( - \dfrac{\| x_1 - x_2 \|^2}{2\sigma^2} \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(x_1, x_2 \in R^d\)</span> are 1-d tensors.</p>
<p>Gaussian Kernel Matrix K is defined on input group <span class="math notranslate nohighlight">\(X=(x_1, x_2, ..., x_m)\)</span></p>
<div class="math notranslate nohighlight">
\[K(X)_{i,j} = k(x_i, x_j)\]</div>
<p>Also by default, during training this layer keeps running estimates of the
mean of L2 distances, which are then used to set hyperparameter  <span class="math notranslate nohighlight">\(\sigma\)</span>.
The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and use a fixed <span class="math notranslate nohighlight">\(\sigma\)</span> instead.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\sigma^2_\text{new} = (1 - \text{momentum}) \times \sigma^2 + \text{momentum} \times \sigma^2_t\)</span>,
where <span class="math notranslate nohighlight">\(\sigma^2_\text{new}\)</span> is the estimated statistic and
<span class="math notranslate nohighlight">\(\sigma^2_t = \dfrac{\alpha}{n^2}\sum_{i,j} \| x_i - x_j \|^2\)</span> is the
new observed value.</p>
</div>
<p>Parameters:</p>
<blockquote>
<div><ul class="simple">
<li><p>sigma (float, optional): bandwidth <span class="math notranslate nohighlight">\(\sigma\)</span>. Default: None</p></li>
<li><p>momentum (float, optional): value used for the <span class="math notranslate nohighlight">\(\sigma^2\)</span> computation. Default: 0.1</p></li>
<li><dl class="simple">
<dt>track_running_stats (bool, optional): If <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module tracks the running mean of <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</dt><dd><p>Otherwise, this module does not track such statistics and always uses fix <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
</dl>
</li>
<li><p>alpha (float, optional): decide the magnitude of <span class="math notranslate nohighlight">\(\sigma^2\)</span> when track_running_stats is set to <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, F)\)</span> where F means the dimension of input features.</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, N)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="dann">
<h2>DANN<a class="headerlink" href="#dann" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="dalib.adaptation.dann.DomainAdversarialLoss">
<em class="property">class </em><code class="sig-prename descclassname">dalib.adaptation.dann.</code><code class="sig-name descname">DomainAdversarialLoss</code><span class="sig-paren">(</span><em class="sig-param">domain_discriminator</em>, <em class="sig-param">reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalib/adaptation/dann.html#DomainAdversarialLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalib.adaptation.dann.DomainAdversarialLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The <a class="reference external" href="https://arxiv.org/abs/1505.07818">Domain Adversarial Loss</a></p>
<p>Domain adversarial loss measures the domain discrepancy through training a domain discriminator.</p>
<dl>
<dt>Parameters:</dt><dd><ul class="simple">
<li><p>domain_discriminator (class:<cite>nn.Module</cite> object): A domain discriminator object, which predicts the domains             of features. Its input shape is (N, F) and output shape is (N, 1)</p></li>
<li><p>reduction (string, optional): Specifies the reduction to apply to the output:             <code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,             <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of             elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>             and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,             specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>f_s, f_t: <span class="math notranslate nohighlight">\((N, F)\)</span> where F means the dimension of input features.</p></li>
<li><p>Output: scalar by default. If :attr:<code class="docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math notranslate nohighlight">\((N, )\)</span>.</p></li>
</ul>
</dd>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">DomainDiscriminator</span><span class="p">(</span><span class="n">in_feature</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">DomainAdversarialLoss</span><span class="p">(</span><span class="n">discriminator</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># features from source domain and target domain</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f_s</span><span class="p">,</span> <span class="n">f_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">f_s</span><span class="p">,</span> <span class="n">f_t</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="dalib.adaptation.dann.DomainDiscriminator">
<em class="property">class </em><code class="sig-prename descclassname">dalib.adaptation.dann.</code><code class="sig-name descname">DomainDiscriminator</code><span class="sig-paren">(</span><em class="sig-param">in_feature</em>, <em class="sig-param">hidden_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalib/adaptation/dann.html#DomainDiscriminator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalib.adaptation.dann.DomainDiscriminator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Domain discriminator model from
<a class="reference external" href="https://arxiv.org/abs/1505.07818">“Domain-Adversarial Training of Neural Networks”</a></p>
<p>Distinguish whether the features with input size (N, F) come from the source domain or the target domain.
The source domain label is 1 and the target domain label is 0.</p>
<dl class="simple">
<dt>Parameters:</dt><dd><ul class="simple">
<li><p>in_feature (int): dimension of the input feature</p></li>
<li><p>hidden_size (int): dimension of the hidden features</p></li>
</ul>
</dd>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, F)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, 1)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="cdan">
<h2>CDAN<a class="headerlink" href="#cdan" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="dalib.adaptation.cdan.ConditionalDomainAdversarialLoss">
<em class="property">class </em><code class="sig-prename descclassname">dalib.adaptation.cdan.</code><code class="sig-name descname">ConditionalDomainAdversarialLoss</code><span class="sig-paren">(</span><em class="sig-param">domain_discriminator</em>, <em class="sig-param">entropy_conditioning=False</em>, <em class="sig-param">randomized=False</em>, <em class="sig-param">num_classes=-1</em>, <em class="sig-param">features_dim=-1</em>, <em class="sig-param">randomized_dim=1024</em>, <em class="sig-param">reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalib/adaptation/cdan.html#ConditionalDomainAdversarialLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalib.adaptation.cdan.ConditionalDomainAdversarialLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The <a class="reference external" href="https://arxiv.org/abs/1705.10667">Conditional Domain Adversarial Loss</a></p>
<p>Conditional Domain adversarial loss measures the domain discrepancy through training a domain discriminator in a conditional manner.</p>
<dl class="simple">
<dt>Parameters:</dt><dd><ul class="simple">
<li><p>domain_discriminator (class:<cite>nn.Module</cite> object): A domain discriminator object, which predicts the domains of             features. Its input shape is (N, F) and output shape is (N, 1)</p></li>
<li><p>entropy_conditioning (bool, optional): If True, use entropy-aware weight to reweight each training             example. Default: False</p></li>
<li><p>randomized (bool, optional): If True, use randomized multi linear map. Else, use multi linear map.             Default: False</p></li>
<li><p>num_classes (int, optional): Number of classes</p></li>
<li><p>features_dim (int, optional): Dimension of input features</p></li>
<li><p>randomized_dim (int, optional): Dimension of features after randomized. Default: 1024</p></li>
<li><p>reduction (string, optional): Specifies the reduction to apply to the output:             <code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,             <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of             elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>             and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,             specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You need to provide num_classes, features_dim and randomized_dim <cite>only when</cite> randomized
is set True.</p>
</div>
<dl>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>y_s, y_t: <span class="math notranslate nohighlight">\((N, C)\)</span> where C means the number of classes.</p></li>
<li><p>f_s, f_t: <span class="math notranslate nohighlight">\((N, F)\)</span> where F means the dimension of input features.</p></li>
<li><p>Output: scalar by default. If :attr:<code class="docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math notranslate nohighlight">\((N, )\)</span>.</p></li>
</ul>
</dd>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature_dim</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">DomainDiscriminator</span><span class="p">(</span><span class="n">in_feature</span><span class="o">=</span><span class="n">feature_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">ConditionalDomainAdversarialLoss</span><span class="p">(</span><span class="n">discriminator</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># features from source domain and target domain</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f_s</span><span class="p">,</span> <span class="n">f_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># logits output from source domain adn target domain</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_s</span><span class="p">,</span> <span class="n">y_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">y_s</span><span class="p">,</span> <span class="n">f_s</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">f_t</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="dalib.adaptation.cdan.DomainDiscriminator">
<em class="property">class </em><code class="sig-prename descclassname">dalib.adaptation.cdan.</code><code class="sig-name descname">DomainDiscriminator</code><span class="sig-paren">(</span><em class="sig-param">in_feature</em>, <em class="sig-param">hidden_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalib/adaptation/cdan.html#DomainDiscriminator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalib.adaptation.cdan.DomainDiscriminator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Domain discriminator model. See class:<cite>dalib.adaptation.dann.DomainDiscriminator</cite> for details.</p>
</dd></dl>

</div>
<div class="section" id="mdd">
<h2>MDD<a class="headerlink" href="#mdd" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="dalib.adaptation.mdd.MarginDisparityDiscrepancyLoss">
<em class="property">class </em><code class="sig-prename descclassname">dalib.adaptation.mdd.</code><code class="sig-name descname">MarginDisparityDiscrepancyLoss</code><span class="sig-paren">(</span><em class="sig-param">adversarial_classifier</em>, <em class="sig-param">margin=2.0</em>, <em class="sig-param">reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalib/adaptation/mdd.html#MarginDisparityDiscrepancyLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalib.adaptation.mdd.MarginDisparityDiscrepancyLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The margin disparity discrepancy (MDD) Loss</p>
<dl class="simple">
<dt>Parameters:</dt><dd><ul class="simple">
<li><p>adversarial_classifier (class:<cite>nn.Module</cite> object): A classifier head which tries to maximize the margin disparity discrepancy.</p></li>
<li><p>margin (float, optional):  margin gamma. Default: 2</p></li>
<li><p>reduction (string, optional): Specifies the reduction to apply to the output:             <code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,             <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of             elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>             and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,             specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>MarginDisparityDiscrepancyLoss has already used GradientReverseLayer, thus adversarial_classifier is
merely a classifier head, e.g. <cite>dalib.adaptation.mdd.AdversarialClassifier</cite></p>
</div>
<dl>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>y_s, y_t: <span class="math notranslate nohighlight">\((N, C)\)</span> where C = number of classes.</p></li>
<li><p>f_s, f_t: <span class="math notranslate nohighlight">\((N, F)\)</span> where F means the dimension of input features.</p></li>
<li><p>Output: scalar. If reduction is ‘none’, then <cite>(N)</cite></p></li>
</ul>
</dd>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature_dim</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">AdversarialClassifier</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">bottleneck_dim</span><span class="o">=</span><span class="n">feature_dim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">MarginDisparityDiscrepancy</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># features from source domain and target domain</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f_s</span><span class="p">,</span> <span class="n">f_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># logits output from source domain adn target domain</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_s</span><span class="p">,</span> <span class="n">y_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">y_s</span><span class="p">,</span> <span class="n">f_s</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">f_t</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="dalib.adaptation.mdd.MarginDisparityDiscrepancy">
<em class="property">class </em><code class="sig-prename descclassname">dalib.adaptation.mdd.</code><code class="sig-name descname">MarginDisparityDiscrepancy</code><span class="sig-paren">(</span><em class="sig-param">margin</em>, <em class="sig-param">reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalib/adaptation/mdd.html#MarginDisparityDiscrepancy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalib.adaptation.mdd.MarginDisparityDiscrepancy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The margin disparity discrepancy (MDD) is proposed to measure the distribution discrepancy in domain adaptation.
The definition can be described as:</p>
<p>You can see more details in <cite>Bridging Theory and Algorithm for Domain Adaptation</cite>
Parameters:</p>
<blockquote>
<div><p>margin (float): margin gamma.</p>
</div></blockquote>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C)\)</span> where C = number of classes.</p></li>
<li><p>Output: scalar. If reduction is ‘none’, then <cite>(N)</cite></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="dalib.adaptation.mdd.AdversarialClassifier">
<em class="property">class </em><code class="sig-prename descclassname">dalib.adaptation.mdd.</code><code class="sig-name descname">AdversarialClassifier</code><span class="sig-paren">(</span><em class="sig-param">in_features</em>, <em class="sig-param">num_classes</em>, <em class="sig-param">bottleneck_dim</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalib/adaptation/mdd.html#AdversarialClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalib.adaptation.mdd.AdversarialClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">dalib.adaptation.mdd.ClassifierHead</span></code></p>
</dd></dl>

<dl class="class">
<dt id="dalib.adaptation.mdd.Classifier">
<em class="property">class </em><code class="sig-prename descclassname">dalib.adaptation.mdd.</code><code class="sig-name descname">Classifier</code><span class="sig-paren">(</span><em class="sig-param">backbone</em>, <em class="sig-param">num_classes</em>, <em class="sig-param">use_bottleneck=True</em>, <em class="sig-param">bottleneck_dim=1024</em>, <em class="sig-param">head_bottleneck_dim=1024</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalib/adaptation/mdd.html#Classifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalib.adaptation.mdd.Classifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Classifier for MDD. Similar as <cite>nn.dalib.vision.classifier.Classifier</cite></p>
</dd></dl>

</div>
<div class="section" id="afn">
<h2>AFN<a class="headerlink" href="#afn" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="dalib.adaptation.afn.StepwiseAdaptiveFeatureNorm">
<em class="property">class </em><code class="sig-prename descclassname">dalib.adaptation.afn.</code><code class="sig-name descname">StepwiseAdaptiveFeatureNorm</code><span class="sig-paren">(</span><em class="sig-param">delta_r=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dalib/adaptation/afn.html#StepwiseAdaptiveFeatureNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dalib.adaptation.afn.StepwiseAdaptiveFeatureNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Stepwise Adaptive Feature Norm proposed by <a class="reference external" href="https://arxiv.xilesou.top/abs/1811.07456">Larger Norm More Transferable:
An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation</a></p>
<dl>
<dt>Parameters:</dt><dd><ul class="simple">
<li><p>delta_r (float, optional): step size of <span class="math notranslate nohighlight">\(\Delta r\)</span>.  Default: 1.</p></li>
</ul>
</dd>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, F)\)</span> where F means the dimension of input features.</p></li>
<li><p>Output: scalar.</p></li>
</ul>
</dd>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">StepwiseAdaptiveFeatureNorm</span><span class="p">(</span><span class="n">delta_r</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># features from source domain or target domain</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">features</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">feautures</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="dalib.vision.datasets.html" class="btn btn-neutral float-right" title="Vision Datasets" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="dalib.modules.html" class="btn btn-neutral float-left" title="DALIB Basic Modules" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, jiangjunguang, fubo

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>